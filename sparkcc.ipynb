{"cells":[{"cell_type":"code","source":["dependencies = [\"warcio\", \"beautifulsoup4\", \"spacy\", \"lxml\", \"langdetect\"]\n#display(dbutils.fs.ls(\"./FileStore/df\"))\n\nfor dependency in dependencies:\n  dbutils.library.installPyPI(dependency)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["%sh\n/databricks/python3/bin/pip install spacy \n/databricks/python3/bin/python3 -m spacy download en_core_web_sm"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Collecting spacy\n  Using cached https://files.pythonhosted.org/packages/55/24/70c615f5b22440c679a4132b81eee67d1dfd70d159505a28ff949c78a1ac/spacy-2.3.2-cp37-cp37m-manylinux1_x86_64.whl\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /databricks/python3/lib/python3.7/site-packages (from spacy) (2.21.0)\nCollecting wasabi&lt;1.1.0,&gt;=0.4.0 (from spacy)\nRequirement already satisfied: numpy&gt;=1.15.0 in /databricks/python3/lib/python3.7/site-packages (from spacy) (1.16.2)\nCollecting catalogue&lt;1.1.0,&gt;=0.0.7 (from spacy)\n  Using cached https://files.pythonhosted.org/packages/6c/f9/9a5658e2f56932e41eb264941f9a2cb7f3ce41a80cb36b2af6ab78e2f8af/catalogue-1.0.0-py2.py3-none-any.whl\nCollecting preshed&lt;3.1.0,&gt;=3.0.2 (from spacy)\n  Using cached https://files.pythonhosted.org/packages/6c/5b/ae4da6230eb48df353b199f53532c8407d0e9eb6ed678d3d36fa75ac391c/preshed-3.0.2-cp37-cp37m-manylinux1_x86_64.whl\nCollecting murmurhash&lt;1.1.0,&gt;=0.28.0 (from spacy)\n  Using cached https://files.pythonhosted.org/packages/73/fc/10eeacb926ec1e88cd62f79d9ac106b0a3e3fe5ff1690422d88c29bd0909/murmurhash-1.0.2-cp37-cp37m-manylinux1_x86_64.whl\nCollecting tqdm&lt;5.0.0,&gt;=4.38.0 (from spacy)\n  Using cached https://files.pythonhosted.org/packages/af/88/7b0ea5fa8192d1733dea459a9e3059afc87819cb4072c43263f2ec7ab768/tqdm-4.48.0-py2.py3-none-any.whl\nCollecting cymem&lt;2.1.0,&gt;=2.0.2 (from spacy)\n  Using cached https://files.pythonhosted.org/packages/e1/79/6ce05ecf4d50344e29749ea7db7ddf427589228fb8fe89b29718c38c27c5/cymem-2.0.3-cp37-cp37m-manylinux1_x86_64.whl\nRequirement already satisfied: setuptools in /usr/lib/python3.7/site-packages (from spacy) (40.8.0)\nCollecting blis&lt;0.5.0,&gt;=0.4.0 (from spacy)\n  Using cached https://files.pythonhosted.org/packages/0a/8c/f1b2aad385de78db151a6e9728026f311dee8bd480f2edc28a0175a543b6/blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl\nCollecting plac&lt;1.2.0,&gt;=0.9.6 (from spacy)\n  Using cached https://files.pythonhosted.org/packages/86/85/40b8f66c2dd8f4fd9f09d59b22720cffecf1331e788b8a0cab5bafb353d1/plac-1.1.3-py2.py3-none-any.whl\nCollecting srsly&lt;1.1.0,&gt;=1.0.2 (from spacy)\n  Using cached https://files.pythonhosted.org/packages/4c/27/0e1deb477dd422427a18d8283b7aacf48b5f77c668feeb2c4920ee6cc3a3/srsly-1.0.2-cp37-cp37m-manylinux1_x86_64.whl\nCollecting thinc==7.4.1 (from spacy)\n  Using cached https://files.pythonhosted.org/packages/1b/4e/6f16cfebb0dd68cc8a9a973eba8e01ee7b960dc563edb12a3ee397473e32/thinc-7.4.1-cp37-cp37m-manylinux1_x86_64.whl\nRequirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /databricks/python3/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (3.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (2019.3.9)\nRequirement already satisfied: urllib3&lt;1.25,&gt;=1.21.1 in /databricks/python3/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (1.24.1)\nRequirement already satisfied: idna&lt;2.9,&gt;=2.5 in /databricks/python3/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (2.8)\nCollecting importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34; (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy)\n  Using cached https://files.pythonhosted.org/packages/8e/58/cdea07eb51fc2b906db0968a94700866fc46249bdc75cac23f9d13168929/importlib_metadata-1.7.0-py2.py3-none-any.whl\nCollecting zipp&gt;=0.5 (from importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34;-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy)\n  Using cached https://files.pythonhosted.org/packages/b2/34/bfcb43cc0ba81f527bc4f40ef41ba2ff4080e047acb0586b56b3d017ace4/zipp-3.1.0-py3-none-any.whl\nInstalling collected packages: wasabi, zipp, importlib-metadata, catalogue, cymem, murmurhash, preshed, tqdm, blis, plac, srsly, thinc, spacy\nSuccessfully installed blis-0.4.1 catalogue-1.0.0 cymem-2.0.3 importlib-metadata-1.7.0 murmurhash-1.0.2 plac-1.1.3 preshed-3.0.2 spacy-2.3.2 srsly-1.0.2 thinc-7.4.1 tqdm-4.48.0 wasabi-0.7.1 zipp-3.1.0\nYou are using pip version 19.0.3, however version 20.2b1 is available.\nYou should consider upgrading via the &#39;pip install --upgrade pip&#39; command.\nCollecting en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0MB)\nRequirement already satisfied: spacy&lt;2.4.0,&gt;=2.3.0 in /databricks/python3/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.2)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /databricks/python3/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (2.21.0)\nRequirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in /databricks/python3/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (0.7.1)\nRequirement already satisfied: numpy&gt;=1.15.0 in /databricks/python3/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.16.2)\nRequirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in /databricks/python3/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.0.0)\nRequirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /databricks/python3/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (3.0.2)\nRequirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /databricks/python3/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.0.2)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /databricks/python3/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (4.48.0)\nRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /databricks/python3/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (2.0.3)\nRequirement already satisfied: setuptools in /usr/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (40.8.0)\nRequirement already satisfied: blis&lt;0.5.0,&gt;=0.4.0 in /databricks/python3/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (0.4.1)\nRequirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in /databricks/python3/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.1.3)\nRequirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in /databricks/python3/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.0.2)\nRequirement already satisfied: thinc==7.4.1 in /databricks/python3/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (7.4.1)\nRequirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /databricks/python3/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (3.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (2019.3.9)\nRequirement already satisfied: urllib3&lt;1.25,&gt;=1.21.1 in /databricks/python3/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.24.1)\nRequirement already satisfied: idna&lt;2.9,&gt;=2.5 in /databricks/python3/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (2.8)\nRequirement already satisfied: importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34; in /databricks/python3/lib/python3.7/site-packages (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.7.0)\nRequirement already satisfied: zipp&gt;=0.5 in /databricks/python3/lib/python3.7/site-packages (from importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34;-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (3.1.0)\nInstalling collected packages: en-core-web-sm\n  Running setup.py install for en-core-web-sm: started\n    Running setup.py install for en-core-web-sm: finished with status &#39;done&#39;\nSuccessfully installed en-core-web-sm-2.3.1\nYou are using pip version 19.0.3, however version 20.2b1 is available.\nYou should consider upgrading via the &#39;pip install --upgrade pip&#39; command.\n<span class=\"ansi-green-fg\">✔ Download and installation successful</span>\nYou can now load the model via spacy.load(&#39;en_core_web_sm&#39;)\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["import argparse\nimport logging\nimport os\nimport re\n\nfrom io import BytesIO\nfrom tempfile import TemporaryFile\n\nimport boto3\nimport botocore\nimport time\nfrom warcio.archiveiterator import ArchiveIterator\nfrom warcio.recordloader import ArchiveLoadFailed\n\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SQLContext, SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType\nfrom multiprocessing.pool import ThreadPool\nimport re\n\nfrom collections import Counter\n\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, ArrayType\n\nfrom bs4 import BeautifulSoup\n\n\n\n\nLOGGING_FORMAT = '%(asctime)s %(levelname)s %(name)s: %(message)s'\n\n\nclass JupyterCCSparkJob(object):\n    \"\"\"\n    A simple Spark job definition to process Common Crawl data\n    \"\"\"\n\n    name = 'CCSparkJob'\n\n    output_schema = StructType([\n        StructField(\"key\", StringType(), True),\n        StructField(\"val\", LongType(), True)\n    ])\n\n    # description of input and output shown in --help\n    input_descr = \"Path to file listing input paths\"\n    output_descr = \"Name of output table (saved in spark.sql.warehouse.dir)\"\n\n    warc_parse_http_header = True\n\n    args = None\n    records_processed = None\n    warc_input_processed = None\n    warc_input_failed = None\n    log_level = 'INFO'\n    logging.basicConfig(level=log_level, format=LOGGING_FORMAT)\n\n\n    num_input_partitions = 400\n    num_output_partitions = 10\n    input_path = \"dbfs:/FileStore/tables/june_2020_warcs.txt\"  #\"/dbfs /FileStore/tables/subset_news_warc.txt\"\n    output_path = \"delete_this\"\n\n\n    def parse_arguments(self):\n        \"\"\" Returns the parsed arguments from the command line \"\"\"\n\n        description = self.name\n        if self.__doc__ is not None:\n            description += \" - \"\n            description += self.__doc__\n        arg_parser = argparse.ArgumentParser(prog=self.name, description=description,\n                                             conflict_handler='resolve')\n\n        arg_parser.add_argument(\"--input\", default=self.input_path, help=self.input_descr)\n        arg_parser.add_argument(\"--output\", default=self.output_path , help=self.output_descr)\n\n        arg_parser.add_argument(\"--num_input_partitions\", type=int,\n                                default=self.num_input_partitions,\n                                help=\"Number of input splits/partitions\")\n        arg_parser.add_argument(\"--num_output_partitions\", type=int,\n                                default=self.num_output_partitions,\n                                help=\"Number of output partitions\")\n        arg_parser.add_argument(\"--output_format\", default=\"parquet\",\n                                help=\"Output format: parquet (default),\"\n                                \" orc, json, csv\")\n        arg_parser.add_argument(\"--output_compression\", default=\"gzip\",\n                                help=\"Output compression codec: None,\"\n                                \" gzip/zlib (default), snappy, lzo, etc.\")\n        arg_parser.add_argument(\"--output_option\", action='append', default=[],\n                                help=\"Additional output option pair\"\n                                \" to set (format-specific) output options, e.g.,\"\n                                \" `header=true` to add a header line to CSV files.\"\n                                \" Option name and value are split at `=` and\"\n                                \" multiple options can be set by passing\"\n                                \" `--output_option <name>=<value>` multiple times\")\n\n        arg_parser.add_argument(\"--local_temp_dir\", default=None,\n                                help=\"Local temporary directory, used to\"\n                                \" buffer content from S3\")\n\n        arg_parser.add_argument(\"--log_level\", default=self.log_level,\n                                help=\"Logging level\")\n        arg_parser.add_argument(\"--spark-profiler\", action='store_true',\n                                help=\"Enable PySpark profiler and log\"\n                                \" profiling metrics if job has finished,\"\n                                \" cf. spark.python.profile\")\n\n        self.add_arguments(arg_parser)\n        args = arg_parser.parse_args(args=[])\n        if not self.validate_arguments(args):\n            raise Exception(\"Arguments not valid\")\n        self.init_logging(args.log_level)\n       # print(args.input)\n        return args\n\n    def add_arguments(self, parser):\n        pass\n\n    def validate_arguments(self, args):\n        if \"orc\" == args.output_format and \"gzip\" == args.output_compression:\n            # gzip for Parquet, zlib for ORC\n            args.output_compression = \"zlib\"\n        return True\n\n    def get_output_options(self):\n        return {x[0]: x[1] for x in map(lambda x: x.split('=', 1),\n                                        self.args.output_option)}\n\n    def init_logging(self, level=None):\n        if level is None:\n            level = self.log_level\n        else:\n            self.log_level = level\n        logging.basicConfig(level=level, format=LOGGING_FORMAT)\n\n    def init_accumulators(self, sc):\n        self.records_processed = sc.accumulator(0)\n        self.warc_input_processed = sc.accumulator(0)\n        self.warc_input_failed = sc.accumulator(0)\n\n    def get_logger(self, spark_context=None):\n        \"\"\"Get logger from SparkContext or (if None) from logging module\"\"\"\n        if spark_context is None:\n            return logging.getLogger(self.name)\n        return spark_context._jvm.org.apache.log4j.LogManager \\\n            .getLogger(self.name)\n\n    def run(self):\n        self.args = self.parse_arguments()\n        \n        conf = SparkConf()\n\n        if self.args.spark_profiler:\n            conf = conf.set(\"spark.python.profile\", \"true\")\n\n        sc = SparkContext.getOrCreate(#appName=self.name,\n          \n            conf=conf)\n        \n        sqlc = SQLContext(sparkContext=sc)\n\n        self.init_accumulators(sc)\n\n        self.run_job(sc, sqlc)\n\n        if self.args.spark_profiler:\n            sc.show_profiles()\n\n        sc.stop()\n\n    def log_aggregator(self, sc, agg, descr):\n        self.get_logger(sc).info(descr.format(agg.value))\n\n    def log_aggregators(self, sc):\n        self.log_aggregator(sc, self.warc_input_processed,\n                            'WARC/WAT/WET input files processed = {}')\n        self.log_aggregator(sc, self.warc_input_failed,\n                            'WARC/WAT/WET input files failed = {}')\n        self.log_aggregator(sc, self.records_processed,\n                            'WARC/WAT/WET records processed = {}')\n\n    @staticmethod\n    def reduce_by_key_func(a, b):\n        return a + b\n\n    def run_job(self, sc, sqlc):\n       \n        input_data = sc.textFile(self.args.input,\n                                 minPartitions=self.args.num_input_partitions)\n        print(\"going to output process warcs\")\n        output = input_data.mapPartitionsWithIndex(self.process_warcs)\n        sqlc.createDataFrame(output, schema=self.output_schema) \\\n            .coalesce(self.args.num_output_partitions) \\\n            .write \\\n            .format(self.args.output_format) \\\n            .option(\"compression\", self.args.output_compression) \\\n            .options(**self.get_output_options()) \\\n            .saveAsTable(self.args.output)\n\n        self.log_aggregators(sc)\n    \n    def process_warcs(self, id_, iterator):\n        s3pattern = re.compile('^s3://([^/]+)/(.+)')\n        base_dir = \"/user/\" #os.path.abspath(os.path.dirname(__file__))\n\n        # S3 client (not thread-safe, initialize outside parallelized loop)\n        no_sign_request = botocore.client.Config(\n            signature_version=botocore.UNSIGNED)\n        s3client = boto3.client('s3', config=no_sign_request)\n       \n        for uri in iterator:\n            self.warc_input_processed.add(1)\n            if uri.startswith('s3://'):\n                self.get_logger().info('Reading from S3 {}'.format(uri))\n                s3match = s3pattern.match(uri)\n                if s3match is None:\n                    self.get_logger().error(\"Invalid S3 URI: \" + uri)\n                    continue\n                bucketname = s3match.group(1)\n                path = s3match.group(2)\n                warctemp = TemporaryFile(mode='w+b',\n                                         dir=self.args.local_temp_dir)\n                try:\n                    s3client.download_fileobj(bucketname, path, warctemp)\n                except botocore.client.ClientError as exception:\n                    self.get_logger().error(\n                        'Failed to download {}: {}'.format(uri, exception))\n                    self.warc_input_failed.add(1)\n                    warctemp.close()\n                    continue\n                warctemp.seek(0)\n                stream = warctemp\n            elif uri.startswith('hdfs://'):\n                self.get_logger().error(\"HDFS input not implemented: \" + uri)\n                continue\n            else:\n                self.get_logger().info('Reading local stream {}'.format(uri))\n                if uri.startswith('file:'):\n                    uri = uri[5:]\n                uri = os.path.join(base_dir, uri)\n                try:\n                    stream = open(uri, 'rb')\n                except IOError as exception:\n                    self.get_logger().error(\n                        'Failed to open {}: {}'.format(uri, exception))\n                    self.warc_input_failed.add(1)\n                    continue\n\n            no_parse = (not self.warc_parse_http_header)\n            end = time.time()\n            print(\"ended with : \", end-start)\n            try:\n                archive_iterator = ArchiveIterator(stream,\n                                                   no_record_parse=no_parse)\n                \n                for res in self.iterate_records(uri, archive_iterator):\n\n                    yield res\n            except ArchiveLoadFailed as exception:\n                self.warc_input_failed.add(1)\n                self.get_logger().error(\n                    'Invalid WARC: {} - {}'.format(uri, exception))\n            finally:\n                stream.close()\n\n    def process_record(self, record):\n        raise NotImplementedError('Processing record needs to be customized')\n\n    def iterate_records(self, _warc_uri, archive_iterator):\n        \"\"\"Iterate over all WARC records. This method can be customized\n           and allows to access also values from ArchiveIterator, namely\n           WARC record offset and length.\"\"\"\n    \n        for record in archive_iterator:\n            for res in self.process_record(record):\n              \n            \n                yield res\n         \n\n            self.records_processed.add(1)\n            # WARC record offset and length should be read after the record\n            # has been processed, otherwise the record content is consumed\n            # while offset and length are determined:\n            #  warc_record_offset = archive_iterator.get_record_offset()\n            #  warc_record_length = archive_iterator.get_record_length()\n\n    @staticmethod\n    def is_wet_text_record(record):\n        \"\"\"Return true if WARC record is a WET text/plain record\"\"\"\n        return (record.rec_type == 'conversion' and\n                record.content_type == 'text/plain')\n\n    @staticmethod\n    def is_wat_json_record(record):\n        \"\"\"Return true if WARC record is a WAT record\"\"\"\n        return (record.rec_type == 'metadata' and\n                record.content_type == 'application/json')\n\n    @staticmethod\n    def is_html(record):\n        \"\"\"Return true if (detected) MIME type of a record is HTML\"\"\"\n        html_types = ['text/html', 'application/xhtml+xml']\n        if (('WARC-Identified-Payload-Type' in record.rec_headers) and\n            (record.rec_headers['WARC-Identified-Payload-Type'] in\n             html_types)):\n            return True\n        for html_type in html_types:\n            if html_type in record.content_type:\n                return True\n        return False\n      \n#Reading from seeds.txt which contains a list of domains of investing / private equity related news articles\nlist_of_domains = []\nwith open(\"/dbfs/FileStore/tables/seeds-1.txt\", \"r\") as f:\n    for line in f:\n        stripped_line = line.split()\n        list_of_domains.append(stripped_line[0])\n\n#These domains don't start consistently with https:// or http://, so we inject the one that is missing\nlist_length = len(list_of_domains)\nfor i in range(list_length):\n  if list_of_domains[i].startswith('https://'):\n    list_of_domains.append(list_of_domains[i].replace('https://','http://'))\n  elif list_of_domains[i].startswith('http://'):\n    list_of_domains.append(list_of_domains[i].replace('http://','https://'))\nprint(\"load the domains: \", len(list_of_domains))\n  \nclass StringMatchCountJob(JupyterCCSparkJob):\n    \"\"\" Word count (frequency list) from texts in Common Crawl WET files\"\"\"\n\n    name = \"StringMatchCount\"\n\n    output_schema = StructType([\n        StructField(\"title\", StringType(), True),\n        StructField(\"body\", StringType(), True),\n        StructField(\"url\", StringType(), True),\n        StructField(\"record_date\", StringType(), True),\n        StructField(\"content_length\", StringType(), True),\n        StructField(\"warc_ip\", StringType(), True),\n        StructField(\"warc_truncated\", StringType(), True),\n\n        StructField(\"server_name\", StringType(), True),\n        StructField(\"vary\", StringType(), True),\n        StructField(\"cache_control\", StringType(), True),\n        StructField(\"content_type\", StringType(), True),\n        StructField(\"http_date\", StringType(), True),\n        StructField(\"http_date_expires\", StringType(), True),\n        StructField(\"html_meta\", ArrayType(StringType()), True),\n        StructField(\"html_hyperlink\", ArrayType(StringType()), True),\n        StructField(\"html_image\", ArrayType(StringType()), True)\n    ]\n\n    )\n\n    # simple Unicode-aware tokenization\n    # (not suitable for CJK languages)\n\n    word_pattern = re.compile('\\w+', re.UNICODE)\n    \n    def process_record(self, record):\n        #check domain\n        proceed = False\n        temp_url = record.rec_headers.get_header('WARC-Target-URI',None)\n        if not temp_url:\n          return\n        for n in list_of_domains:\n          if n in temp_url:\n            proceed = True\n            break\n        proceed = True\n        if not proceed:\n          return\n        \n        if record.rec_type == 'response':\n            # record headers\n\n            record_url = record.rec_headers.get_header('WARC-Target-URI', None)\n            record_date = record.rec_headers.get_header('WARC-Date', None)\n            record_content_length = record.rec_headers.get_header('Content-Length', None)\n            record_ip = record.rec_headers.get_header('WARC-IP-Address', None)\n            record_truncated = record.rec_headers.get_header('WARC-Truncated', None)\n\n            # http headers\n\n            http_server_name = record.http_headers.get_header('Server', None)\n            http_vary = record.http_headers.get_header('Vary', None)\n            http_cache_control = record.http_headers.get_header('Cache-Control', None)\n            http_content_type = record.http_headers.get_header('Content-Type', None)\n            http_date = record.http_headers.get_header('Date', None)\n            http_expires = record.http_headers.get_header('Expires', None)\n\n            data = record.content_stream().read()\n\n            soup = BeautifulSoup(data, 'lxml')\n\n\n            title = soup.find('title').string if soup.find('title') else None\n            if not title:\n                return\n            paragraphs = soup.find_all(['p', 'b'])\n\n            whitelist = ['meta', 'a', 'img']\n            html_meta = []\n            html_hyperlink = []\n            html_image = []\n            for t in soup.find_all(whitelist):\n                if t.name == 'meta':\n                    html_meta.append(str(t))\n                elif t.name == 'a':\n                    html_hyperlink.append(str(t))\n                elif t.name == 'img':\n                    html_image.append(str(t))\n\n            body = \"\"\n            for paragraph in paragraphs:\n                if not paragraph.has_attr('class'):\n                    body += paragraph.getText() + \" \"\n\n            string_title = str(title)\n\n            yield string_title, body, record_url, record_date, record_content_length, record_ip,\\\n                  record_truncated, http_server_name, http_vary, http_cache_control, http_content_type, \\\n                  http_date, http_expires, html_meta, html_hyperlink, html_image\n        else:\n            return\n#dbutils.fs.rm('/user/hive/warehouse/domain_test_delete', True)\n\n\njob = StringMatchCountJob()\n\n\njob.run()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ModuleNotFoundError</span>                       Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1396796701613474&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     10</span> <span class=\"ansi-green-fg\">import</span> botocore\n<span class=\"ansi-green-intense-fg ansi-bold\">     11</span> <span class=\"ansi-green-fg\">import</span> time\n<span class=\"ansi-green-fg\">---&gt; 12</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">from</span> warcio<span class=\"ansi-blue-fg\">.</span>archiveiterator <span class=\"ansi-green-fg\">import</span> ArchiveIterator\n<span class=\"ansi-green-intense-fg ansi-bold\">     13</span> <span class=\"ansi-green-fg\">from</span> warcio<span class=\"ansi-blue-fg\">.</span>recordloader <span class=\"ansi-green-fg\">import</span> ArchiveLoadFailed\n<span class=\"ansi-green-intense-fg ansi-bold\">     14</span> \n\n<span class=\"ansi-red-fg\">ModuleNotFoundError</span>: No module named &#39;warcio&#39;</div>"]}}],"execution_count":3},{"cell_type":"code","source":["submarkets = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/submarkets.csv')\n#display(dbutils.fs.ls('FileStore/tables'))\n#display(submarkets)\nset_of_companies = []\n\nfor row in submarkets.collect():\n  set_of_companies.append(row['name'])\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["#Run this to create a directory that the next cell will dump parquets into\ndbutils.fs.mkdirs(\"FileStore/tables/test/\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[146]: True</div>"]}}],"execution_count":5},{"cell_type":"code","source":["\n\nimport argparse\nimport logging\nimport os\nimport re\n\nfrom io import BytesIO\nfrom tempfile import TemporaryFile\n\nimport boto3\nimport botocore\n#from pyspark.python.pyspark.shell import spark\n\nfrom warcio.archiveiterator import ArchiveIterator\nfrom warcio.recordloader import ArchiveLoadFailed\n\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SQLContext, SparkSession\nfrom pyspark.sql.functions import udf, col, lit\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, BooleanType\nimport argparse\nimport spacy\nimport pandas as pd\nfrom tqdm import tqdm\nimport time\nfrom langdetect import detect\n\nconf = SparkConf()\nsc = SparkContext.getOrCreate(conf=conf)\nsqlc = SQLContext(sparkContext=sc)\ndf = sqlc.read.parquet(\"user/hive/warehouse/hackathon_june_2020\")\nnlp = spacy.load('en_core_web_sm')\nlogging.getLogger(\"py4j\").setLevel(logging.ERROR)\nrows = []\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--ner', dest='ner', action='store_true')\nparser.add_argument('--no-ner', dest='ner', action='store_false')\nparser.add_argument('--csv', dest='csv', action='store_true')\nparser.add_argument('--no-csv', dest='csv', action='store_false')\nparser.set_defaults(ner=False)\nparser.set_defaults(csv=True)\nargs = parser.parse_args(args=[])\n\nlist_of_domains = []\nlist_of_companies = []\nsubmarkets = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/submarkets.csv')\n\n#Reading from seeds.txt which contains a list of domains of investing / private equity related news articles\nwith open(\"/dbfs/FileStore/tables/seeds.txt\", \"r\") as f:\n    for line in f:\n        stripped_line = line.split()\n        list_of_domains.append(stripped_line[0])\n\n#These domains don't start consistently with https:// or http://, so we inject the one that is missing\nlist_length = len(list_of_domains)\nfor i in range(list_length):\n  if list_of_domains[i].startswith('https://'):\n    list_of_domains.append(list_of_domains[i].replace('https://','http://'))\n  elif list_of_domains[i].startswith('http://'):\n    list_of_domains.append(list_of_domains[i].replace('http://','https://'))\n    \n\nfor row in submarkets.collect():\n  list_of_companies.append(row['name'])\n\n\n\n\n#Name entity recognition with spacy, on the title\ndef perform_NER_title(title, query):\n    doc = nlp(title)\n    for ent in doc.ents:\n        if ent.label_ == \"ORG\" and ent.text.lower() in query.lower():\n            return True\n    return False\n\n#Name entity recognition with spacy, on the body\ndef perform_NER_body(body, query):\n    doc = nlp(body)\n    for ent in doc.ents:\n        if ent.label_ == \"ORG\" and ent.text.lower() in query.lower():\n            return True\n    return False\n\n#We check if any of the seed domains match with the current record, if it matches that means it's likely that it is important\ndef domain_match(url):\n    for n in list_of_domains:\n        if n in url:\n            return True\n    return False\n  \n#Tokenize the title string and check if any of the tokens is equal to our company of interest\ndef token_match_title(title, query):\n    title_tokens = title.split()\n    for n in title_tokens:\n        if n == query:\n            return True\n    return False\n\n#Tokenize the body string and check if any of the tokens is equal to our company of interest\ndef token_match_body(body, query):\n    body_tokens = body.split()\n    for n in body_tokens:\n        if n == query:\n            return True\n    return False\n\n#Substring match on the title string\ndef string_match_title(title, query):\n    if query in title:\n        return True\n    return False\n\n#Substring match on the body string\ndef string_match_body(body, query):\n    if query in body:\n        return True\n    return False\n\n#We know that substring match on body / title supersets everything else, so we perform this initial filter to reduce search space of other filters\ndef performFilter(title, body, query):\n    if string_match_title(title,query):\n        return True\n    return False\n  \n  \n    if string_match_body(body, query) or string_match_title(title, query): #body and title string match supercedes all\n        return True\n    return False\n\n#Returning which company that we are matching on\ndef company_match(query):\n    return query\n  \ndef detect_language(title):\n  try:\n    lang = detect(title)\n  except Exception as e:\n    return \"No Lang\"\n \n  return lang\n\n#UDF definitions\nfilter_udf = udf(performFilter, BooleanType())\ndomain_match_udf = udf(domain_match, BooleanType())\ntitle_match_token_udf = udf(token_match_title, BooleanType())\nbody_match_token_udf = udf(token_match_body, BooleanType())\ntitle_match_string_udf = udf(string_match_title, BooleanType())\nbody_match_string_udf = udf(string_match_body, BooleanType())\nner_title_udf = udf(perform_NER_title, BooleanType())\nner_body_udf = udf(perform_NER_body, BooleanType())\ncompany_match_udf = udf(company_match, StringType())\ndetect_language_udf = udf(detect_language, StringType())\n\nprocessed_companies = set()\n\ndef performQuery(n):\n    n = n.replace(\" \",\"_\").replace(\"(\",\"[\").replace(\")\",\"]\")\n    if n == \"Florence_[Business/Productivity_Software]\" :\n      n = \"Florence\"\n      \n    tmp_path = f'/FileStore/tables/hackathon_june_2020/{n}'\n    \n    if n in processed_companies:\n      print(f\"{n} already saved!\")\n      return True\n    \n    \n    master_df = None\n    start = time.time()\n    #Add in bool columns for each type of filter\n     \n    filtered_df = df.filter(df.body.contains(n) | df.title.contains(n) )\n    \n    filtered_df = filtered_df.withColumn(\"domain_match\", domain_match_udf(\"url\"))\n    filtered_df = filtered_df.withColumn(\"title_match_string\", title_match_string_udf(\"title\", lit(n)))\n    filtered_df = filtered_df.withColumn(\"body_match_string\", body_match_string_udf(\"body\", lit(n)))\n    filtered_df = filtered_df.withColumn(\"title_match_token\", title_match_token_udf(\"title\", lit(n)))\n    filtered_df = filtered_df.withColumn(\"body_match_token\", body_match_token_udf(\"body\", lit(n)))\n    filtered_df = filtered_df.withColumn(\"title_ner\", ner_title_udf(\"title\", lit(n)))\n    filtered_df = filtered_df.withColumn(\"language\", detect_language_udf(\"title\"))\n    \n    #filtered = filtered_df.withColumn(\"body_ner\", ner_body_udf(\"body\", lit(n)))\n    #print(\"Done body_ner: \", time.time() - start)\n    \n    filtered_df = filtered_df.withColumn(\"company_match\", company_match_udf(lit(n)))\n    master_df = filtered_df\n    \n     \n    master_df.write.parquet(tmp_path)\n    \n    #master_df.repartition(32).write.parquet(tmp_path)\n    processed_companies.add(n)\n\n    print(f\"Done saving to csv for {n}\", time.time() - start)\n    return True\n\nfor n in dbutils.fs.ls(\"dbfs:/FileStore/tables/hackathon_june_2020\"):\n  processed_companies.add(n.name[:len(n.name) - 1])\n\nrealstart = time.time()\nfor n in list_of_companies:\n  performQuery(n)\n\n\nprint(\"full time:\" ,time.time() - realstart)\n\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">12Twenty already saved!\n1upHealth already saved!\n2Checkout_[Acquired] already saved!\n3T_Biosciences already saved!\n4G_Clinical already saved!\n500_Miles already saved!\n60_Degrees_Pharma already saved!\n7shifts already saved!\n98point6 already saved!\nA-LIGN_Assurance already saved!\nABA_Access already saved!\nAbacode already saved!\nAbacus_[California] already saved!\nAbalone_Bio already saved!\nAbartys_Health already saved!\nAbCellera already saved!\nAbsenceSoft already saved!\nAbstract already saved!\nAcademy_School_of_Blockchain already saved!\nAcadiate already saved!\nAcacia_Living already saved!\nAccord_Systems already saved!\nAccountedCare already saved!\nAccrualify already saved!\nAcculynk already saved!\nAccupoint_Software_Development already saved!\nAchievers already saved!\nAcorn_Finance already saved!\nACT.md already saved!\nActivEd already saved!\nActivibe already saved!\nActual_Healthcare_Solutions already saved!\nActualize_Therapy already saved!\nActX already saved!\nAdherence_Compliance already saved!\nAdia already saved!\nAdvaita_Bioinformatics already saved!\nAdvanced_ICU_Care already saved!\nAdvekit already saved!\nAdvera_Health_Analytics already saved!\nAdviNow_Medical already saved!\nAdvisor_Credit_Exchange already saved!\nAdvisory_Cloud already saved!\nAegis_Mobile already saved!\nAeroPay already saved!\nAffiniPay already saved!\nAffirm already saved!\nAgBoost already saved!\nAgeCheq already saved!\nAhoona already saved!\nAhvoda_Recruitment already saved!\nAI_Therapeutics already saved!\nAifred_Health already saved!\nAiva_Health already saved!\nAI_VALI already saved!\nAkili_Interactive already saved!\nAlea_Diagnostics already saved!\nAlertEnterprise already saved!\nAlert_Logic already saved!\nAlienVault already saved!\nAligned_TeleHealth already saved!\nAliveCor already saved!\nAlloy_[Business_software] already saved!\nAlloy_[Identity_Verification_APIs] already saved!\nAllVoices.co already saved!\nAllyO already saved!\nLoan_Transactions_&amp;_Technology already saved!\nAlpha_Cares already saved!\nAlternate36 already saved!\nAlto_Neuroscience already saved!\nAltruista_Health already saved!\nAlxerion already saved!\nAlyss_Analytics already saved!\nAmaryllis_Payment_Solutions already saved!\nAmber_Financial_Services already saved!\nAmerican_Financial_Exchange already saved!\nAmideBio already saved!\nAmino_[health_care_platform] already saved!\nAmira_Health already saved!\nAdvanced_Medical_Predictive_Devices,_Diagnostics_and_Displays already saved!\nAmplifAI already saved!\nAmplion already saved!\nAmylyx already saved!\nAnalyst_Hub already saved!\nAnchor_Health already saved!\nAnnai_Systems already saved!\nAnonos already saved!\nAnsaro already saved!\nAntara_Health already saved!\nApostle already saved!\nApostrophe_[Healthcare] already saved!\nApplied_CFD_Technologies already saved!\nAppMedicine already saved!\nAppointment_Launch already saved!\nAppriza_Pay already saved!\nAptible already saved!\naptihealth already saved!\nArbor_Biotechnologies already saved!\nAriel_Precision_Medicine already saved!\nArkose_Labs already saved!\nArmorText already saved!\nArpeggio_Biosciences already saved!\nArterys already saved!\nModern_Labor already saved!\nAscend_HR already saved!\nAscend_Consumer_Finance already saved!\nAskFora already saved!\nPillar_[New_York] already saved!\nTia_[Bot] already saved!\nAspect_Biosystems already saved!\nAssayMe already saved!\nAssembled already saved!\nAsset_Health already saved!\nAstarte_Medical already saved!\nAtentiv already saved!\nAtomwise already saved!\nAuditBoard already saved!\nAura_Health already saved!\nAuspex_Diagnostics already saved!\nAutomox already saved!\nAutonomous_Healthcare already saved!\nAvanan already saved!\nAvant_[Online_Lending_Firm] already saved!\nSurvivorplan already saved!\nAvesta_Systems already saved!\nAvitas_Systems already saved!\nAxial_Healthcare already saved!\nAxio already saved!\nDone saving to csv for Axxess 14.374239683151245\nDone saving to csv for Azimo 10.687071800231934\nDone saving to csv for Azimuth_GRC 9.510469913482666\nDone saving to csv for Azuba 9.627786636352539\nDone saving to csv for Azumio 9.286524772644043\nDone saving to csv for Backpack_Health 9.410021305084229\nDone saving to csv for Bacon_Inc. 10.006699085235596\nDone saving to csv for BalaBit 9.714385747909546\nDone saving to csv for Balbix 9.702032089233398\nDone saving to csv for Bambee 9.16468596458435\nDone saving to csv for BambooHR 9.71878457069397\nDone saving to csv for BankMobile 9.685921907424927\nDone saving to csv for Banyan_Infrastructure 10.135192394256592\nDone saving to csv for BanyanOps 9.287298679351807\nDone saving to csv for Basecamp 10.458984613418579\nDone saving to csv for BaseHealth 9.696925163269043\nDone saving to csv for Bayesian_Health 9.923044919967651\nDone saving to csv for Beam_Health_group 9.388036012649536\nDone saving to csv for Beam_Solutions 9.320563554763794\nDone saving to csv for BearX 9.933083534240723\nDone saving to csv for Beautiful_Day 9.512988805770874\nDone saving to csv for Become_[Financial_Software] 9.4477219581604\nDone saving to csv for BehaveCare 9.694985389709473\nDone saving to csv for BehaveHealth 9.672384262084961\nDone saving to csv for Caring_Technologies 9.48976755142212\nDone saving to csv for Behavox 9.672562599182129\nDone saving to csv for Belong.Life 9.419265747070312\nDone saving to csv for Benefits_Science 9.89684510231018\nDone saving to csv for Benevity 9.729111433029175\nDone saving to csv for Be_Pure_Beauty 9.539025783538818\nDone saving to csv for Besomebody 9.533315658569336\nDone saving to csv for BetterDoctor 9.210906744003296\nDone saving to csv for Better_Than_Cash_Alliance 9.170745134353638\nDone saving to csv for Better_Therapeutics 9.89078974723816\nDone saving to csv for BetterUp 9.549793243408203\nDone saving to csv for Bidder 9.501315832138062\nDone saving to csv for BigHat_Biosciences 9.58009123802185\nDone saving to csv for Big_Health 9.46455717086792\nDone saving to csv for BigID 9.470600843429565\nDone saving to csv for Bill.com 9.76087474822998\nDone saving to csv for BillGO 9.480988264083862\nDone saving to csv for Billtrust 9.002346754074097\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["test_parquet = spark.read.parquet(\"/user/hive/warehouse/domain_test_delete\")\nprint(test_parquet.count())\n#display(test_parquet)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">2020-07-24 19:19:19,118 INFO py4j.java_gateway: Received command c on object id p0\n2020-07-24 19:19:19,118 INFO py4j.java_gateway: Received command c on object id p0\n2020-07-24 19:19:19,118 INFO py4j.java_gateway: Received command c on object id p0\n2020-07-24 19:19:19,119 INFO py4j.java_gateway: Received command c on object id p0\n2020-07-24 19:19:19,119 INFO py4j.java_gateway: Received command c on object id p0\n2020-07-24 19:19:19,119 INFO py4j.java_gateway: Received command c on object id p0\n2020-07-24 19:19:19,119 INFO py4j.java_gateway: Received command c on object id p0\n2020-07-24 19:19:19,121 INFO py4j.java_gateway: Received command c on object id p0\n2020-07-24 19:19:19,218 INFO py4j.java_gateway: Received command c on object id p0\n2557\n2020-07-24 19:19:19,273 INFO py4j.java_gateway: Received command c on object id p0\n2020-07-24 19:19:19,274 INFO py4j.java_gateway: Received command c on object id p0\n2020-07-24 19:19:19,274 INFO py4j.java_gateway: Received command c on object id p0\n2020-07-24 19:19:19,275 INFO py4j.java_gateway: Received command c on object id p0\n2020-07-24 19:19:19,275 INFO py4j.java_gateway: Received command c on object id p0\n2020-07-24 19:19:19,276 INFO py4j.java_gateway: Received command c on object id p0\n2020-07-24 19:19:19,276 INFO py4j.java_gateway: Received command c on object id p0\n2020-07-24 19:19:19,277 INFO py4j.java_gateway: Received command c on object id p0\n2020-07-24 19:19:19,318 INFO py4j.java_gateway: Received command c on object id p0\n2020-07-24 19:19:19,319 INFO py4j.java_gateway: Received command c on object id p0\n2020-07-24 19:19:19,319 INFO py4j.java_gateway: Received command c on object id p0\n2020-07-24 19:19:19,319 INFO py4j.java_gateway: Received command c on object id p0\n2020-07-24 19:19:19,319 INFO py4j.java_gateway: Received command c on object id p0\n2020-07-24 19:19:19,319 INFO py4j.java_gateway: Received command c on object id p0\n2020-07-24 19:19:19,320 INFO py4j.java_gateway: Received command c on object id p0\n2020-07-24 19:19:19,320 INFO py4j.java_gateway: Received command c on object id p0\n2020-07-24 19:19:19,320 INFO py4j.java_gateway: Received command c on object id p0\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["display(dbutils.fs.ls(\"/user/hive/warehouse/domain_test_delete\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/user/hive/warehouse/domain_test_delete/_SUCCESS</td><td>_SUCCESS</td><td>0</td></tr><tr><td>dbfs:/user/hive/warehouse/domain_test_delete/_committed_6106379306745252470</td><td>_committed_6106379306745252470</td><td>974</td></tr><tr><td>dbfs:/user/hive/warehouse/domain_test_delete/_started_6106379306745252470</td><td>_started_6106379306745252470</td><td>0</td></tr><tr><td>dbfs:/user/hive/warehouse/domain_test_delete/part-00000-tid-6106379306745252470-7a0b6902-14fd-43d7-9bda-c39d5b4d411b-88-1-c000.gz.parquet</td><td>part-00000-tid-6106379306745252470-7a0b6902-14fd-43d7-9bda-c39d5b4d411b-88-1-c000.gz.parquet</td><td>850388</td></tr><tr><td>dbfs:/user/hive/warehouse/domain_test_delete/part-00001-tid-6106379306745252470-7a0b6902-14fd-43d7-9bda-c39d5b4d411b-89-1-c000.gz.parquet</td><td>part-00001-tid-6106379306745252470-7a0b6902-14fd-43d7-9bda-c39d5b4d411b-89-1-c000.gz.parquet</td><td>1343589</td></tr><tr><td>dbfs:/user/hive/warehouse/domain_test_delete/part-00002-tid-6106379306745252470-7a0b6902-14fd-43d7-9bda-c39d5b4d411b-90-1-c000.gz.parquet</td><td>part-00002-tid-6106379306745252470-7a0b6902-14fd-43d7-9bda-c39d5b4d411b-90-1-c000.gz.parquet</td><td>354274</td></tr><tr><td>dbfs:/user/hive/warehouse/domain_test_delete/part-00003-tid-6106379306745252470-7a0b6902-14fd-43d7-9bda-c39d5b4d411b-91-1-c000.gz.parquet</td><td>part-00003-tid-6106379306745252470-7a0b6902-14fd-43d7-9bda-c39d5b4d411b-91-1-c000.gz.parquet</td><td>1689540</td></tr><tr><td>dbfs:/user/hive/warehouse/domain_test_delete/part-00004-tid-6106379306745252470-7a0b6902-14fd-43d7-9bda-c39d5b4d411b-92-1-c000.gz.parquet</td><td>part-00004-tid-6106379306745252470-7a0b6902-14fd-43d7-9bda-c39d5b4d411b-92-1-c000.gz.parquet</td><td>277516</td></tr><tr><td>dbfs:/user/hive/warehouse/domain_test_delete/part-00005-tid-6106379306745252470-7a0b6902-14fd-43d7-9bda-c39d5b4d411b-93-1-c000.gz.parquet</td><td>part-00005-tid-6106379306745252470-7a0b6902-14fd-43d7-9bda-c39d5b4d411b-93-1-c000.gz.parquet</td><td>2209223</td></tr><tr><td>dbfs:/user/hive/warehouse/domain_test_delete/part-00006-tid-6106379306745252470-7a0b6902-14fd-43d7-9bda-c39d5b4d411b-94-1-c000.gz.parquet</td><td>part-00006-tid-6106379306745252470-7a0b6902-14fd-43d7-9bda-c39d5b4d411b-94-1-c000.gz.parquet</td><td>1091014</td></tr><tr><td>dbfs:/user/hive/warehouse/domain_test_delete/part-00007-tid-6106379306745252470-7a0b6902-14fd-43d7-9bda-c39d5b4d411b-95-1-c000.gz.parquet</td><td>part-00007-tid-6106379306745252470-7a0b6902-14fd-43d7-9bda-c39d5b4d411b-95-1-c000.gz.parquet</td><td>2942048</td></tr><tr><td>dbfs:/user/hive/warehouse/domain_test_delete/part-00008-tid-6106379306745252470-7a0b6902-14fd-43d7-9bda-c39d5b4d411b-96-1-c000.gz.parquet</td><td>part-00008-tid-6106379306745252470-7a0b6902-14fd-43d7-9bda-c39d5b4d411b-96-1-c000.gz.parquet</td><td>1158770</td></tr><tr><td>dbfs:/user/hive/warehouse/domain_test_delete/part-00009-tid-6106379306745252470-7a0b6902-14fd-43d7-9bda-c39d5b4d411b-97-1-c000.gz.parquet</td><td>part-00009-tid-6106379306745252470-7a0b6902-14fd-43d7-9bda-c39d5b4d411b-97-1-c000.gz.parquet</td><td>3447564</td></tr></tbody></table></div>"]}}],"execution_count":8},{"cell_type":"code","source":["dbutils.fs.mkdirs(\"FileStore/tables/hackathon_june_2020\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: True</div>"]}}],"execution_count":9},{"cell_type":"code","source":["dbutils.fs.rm('dbfs:/FileStore/tables/image_subset', True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: True</div>"]}}],"execution_count":10},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":11}],"metadata":{"name":"sparkcc","notebookId":1396796701613468},"nbformat":4,"nbformat_minor":0}
