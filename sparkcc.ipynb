{"cells":[{"cell_type":"code","source":["dependencies = [\"warcio\", \"beautifulsoup4\", \"spacy\", \"lxml\", \"langdetect\", \"matplotlib\"]\n\nfor dependency in dependencies:\n  dbutils.library.installPyPI(dependency)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["%sh\n/databricks/python3/bin/pip install spacy \n/databricks/python3/bin/python3 -m spacy download en_core_web_sm"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Collecting spacy\n  Using cached https://files.pythonhosted.org/packages/55/24/70c615f5b22440c679a4132b81eee67d1dfd70d159505a28ff949c78a1ac/spacy-2.3.2-cp37-cp37m-manylinux1_x86_64.whl\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /databricks/python3/lib/python3.7/site-packages (from spacy) (2.21.0)\nCollecting wasabi&lt;1.1.0,&gt;=0.4.0 (from spacy)\nRequirement already satisfied: numpy&gt;=1.15.0 in /databricks/python3/lib/python3.7/site-packages (from spacy) (1.16.2)\nCollecting catalogue&lt;1.1.0,&gt;=0.0.7 (from spacy)\n  Using cached https://files.pythonhosted.org/packages/6c/f9/9a5658e2f56932e41eb264941f9a2cb7f3ce41a80cb36b2af6ab78e2f8af/catalogue-1.0.0-py2.py3-none-any.whl\nCollecting preshed&lt;3.1.0,&gt;=3.0.2 (from spacy)\n  Using cached https://files.pythonhosted.org/packages/6c/5b/ae4da6230eb48df353b199f53532c8407d0e9eb6ed678d3d36fa75ac391c/preshed-3.0.2-cp37-cp37m-manylinux1_x86_64.whl\nCollecting murmurhash&lt;1.1.0,&gt;=0.28.0 (from spacy)\n  Using cached https://files.pythonhosted.org/packages/73/fc/10eeacb926ec1e88cd62f79d9ac106b0a3e3fe5ff1690422d88c29bd0909/murmurhash-1.0.2-cp37-cp37m-manylinux1_x86_64.whl\nCollecting tqdm&lt;5.0.0,&gt;=4.38.0 (from spacy)\n  Using cached https://files.pythonhosted.org/packages/28/7e/281edb5bc3274dfb894d90f4dbacfceaca381c2435ec6187a2c6f329aed7/tqdm-4.48.2-py2.py3-none-any.whl\nCollecting cymem&lt;2.1.0,&gt;=2.0.2 (from spacy)\n  Using cached https://files.pythonhosted.org/packages/e1/79/6ce05ecf4d50344e29749ea7db7ddf427589228fb8fe89b29718c38c27c5/cymem-2.0.3-cp37-cp37m-manylinux1_x86_64.whl\nRequirement already satisfied: setuptools in /usr/lib/python3.7/site-packages (from spacy) (40.8.0)\nCollecting blis&lt;0.5.0,&gt;=0.4.0 (from spacy)\n  Using cached https://files.pythonhosted.org/packages/0a/8c/f1b2aad385de78db151a6e9728026f311dee8bd480f2edc28a0175a543b6/blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl\nCollecting plac&lt;1.2.0,&gt;=0.9.6 (from spacy)\n  Using cached https://files.pythonhosted.org/packages/86/85/40b8f66c2dd8f4fd9f09d59b22720cffecf1331e788b8a0cab5bafb353d1/plac-1.1.3-py2.py3-none-any.whl\nCollecting srsly&lt;1.1.0,&gt;=1.0.2 (from spacy)\n  Using cached https://files.pythonhosted.org/packages/4c/27/0e1deb477dd422427a18d8283b7aacf48b5f77c668feeb2c4920ee6cc3a3/srsly-1.0.2-cp37-cp37m-manylinux1_x86_64.whl\nCollecting thinc==7.4.1 (from spacy)\n  Using cached https://files.pythonhosted.org/packages/1b/4e/6f16cfebb0dd68cc8a9a973eba8e01ee7b960dc563edb12a3ee397473e32/thinc-7.4.1-cp37-cp37m-manylinux1_x86_64.whl\nRequirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /databricks/python3/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (3.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (2019.3.9)\nRequirement already satisfied: urllib3&lt;1.25,&gt;=1.21.1 in /databricks/python3/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (1.24.1)\nRequirement already satisfied: idna&lt;2.9,&gt;=2.5 in /databricks/python3/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (2.8)\nCollecting importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34; (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy)\n  Using cached https://files.pythonhosted.org/packages/8e/58/cdea07eb51fc2b906db0968a94700866fc46249bdc75cac23f9d13168929/importlib_metadata-1.7.0-py2.py3-none-any.whl\nCollecting zipp&gt;=0.5 (from importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34;-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy)\n  Using cached https://files.pythonhosted.org/packages/b2/34/bfcb43cc0ba81f527bc4f40ef41ba2ff4080e047acb0586b56b3d017ace4/zipp-3.1.0-py3-none-any.whl\nInstalling collected packages: wasabi, zipp, importlib-metadata, catalogue, cymem, murmurhash, preshed, tqdm, blis, plac, srsly, thinc, spacy\nSuccessfully installed blis-0.4.1 catalogue-1.0.0 cymem-2.0.3 importlib-metadata-1.7.0 murmurhash-1.0.2 plac-1.1.3 preshed-3.0.2 spacy-2.3.2 srsly-1.0.2 thinc-7.4.1 tqdm-4.48.2 wasabi-0.7.1 zipp-3.1.0\nYou are using pip version 19.0.3, however version 20.2.1 is available.\nYou should consider upgrading via the &#39;pip install --upgrade pip&#39; command.\nCollecting en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0MB)\nRequirement already satisfied: spacy&lt;2.4.0,&gt;=2.3.0 in /databricks/python3/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.2)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /databricks/python3/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (2.21.0)\nRequirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in /databricks/python3/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (0.7.1)\nRequirement already satisfied: numpy&gt;=1.15.0 in /databricks/python3/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.16.2)\nRequirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in /databricks/python3/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.0.0)\nRequirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /databricks/python3/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (3.0.2)\nRequirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /databricks/python3/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.0.2)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /databricks/python3/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (4.48.2)\nRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /databricks/python3/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (2.0.3)\nRequirement already satisfied: setuptools in /usr/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (40.8.0)\nRequirement already satisfied: blis&lt;0.5.0,&gt;=0.4.0 in /databricks/python3/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (0.4.1)\nRequirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in /databricks/python3/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.1.3)\nRequirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in /databricks/python3/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.0.2)\nRequirement already satisfied: thinc==7.4.1 in /databricks/python3/lib/python3.7/site-packages (from spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (7.4.1)\nRequirement already satisfied: chardet&lt;3.1.0,&gt;=3.0.2 in /databricks/python3/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (3.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (2019.3.9)\nRequirement already satisfied: urllib3&lt;1.25,&gt;=1.21.1 in /databricks/python3/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.24.1)\nRequirement already satisfied: idna&lt;2.9,&gt;=2.5 in /databricks/python3/lib/python3.7/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (2.8)\nRequirement already satisfied: importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34; in /databricks/python3/lib/python3.7/site-packages (from catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (1.7.0)\nRequirement already satisfied: zipp&gt;=0.5 in /databricks/python3/lib/python3.7/site-packages (from importlib-metadata&gt;=0.20; python_version &lt; &#34;3.8&#34;-&gt;catalogue&lt;1.1.0,&gt;=0.0.7-&gt;spacy&lt;2.4.0,&gt;=2.3.0-&gt;en_core_web_sm==2.3.1) (3.1.0)\nInstalling collected packages: en-core-web-sm\n  Running setup.py install for en-core-web-sm: started\n    Running setup.py install for en-core-web-sm: finished with status &#39;done&#39;\nSuccessfully installed en-core-web-sm-2.3.1\nYou are using pip version 19.0.3, however version 20.2.1 is available.\nYou should consider upgrading via the &#39;pip install --upgrade pip&#39; command.\n<span class=\"ansi-green-fg\">✔ Download and installation successful</span>\nYou can now load the model via spacy.load(&#39;en_core_web_sm&#39;)\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["#Populate list_of_domains with a whitelist of acceptable news article domains\n\nlist_of_domains = []\n\nseed_path = \"/dbfs/FileStore/tables/seeds-1.txt\"\n\n#Reading from seeds.txt which contains a list of domains of investing / private equity related news articles\nwith open(seed_path, \"r\") as f:\n    for line in f:\n        stripped_line = line.split()\n        list_of_domains.append(stripped_line[0])\n\n#These domains don't start consistently with https:// or http://, so we inject the one that is missing\nlist_length = len(list_of_domains)\nfor i in range(list_length):\n  if list_of_domains[i].startswith('https://'):\n    list_of_domains.append(list_of_domains[i].replace('https://','http://'))\n  elif list_of_domains[i].startswith('http://'):\n    list_of_domains.append(list_of_domains[i].replace('http://','https://'))\nlist_of_domains = set(list_of_domains)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#From a list of WARC file paths, this will read all the records from relevant domains and store them in a parquet file\n\nimport argparse\nimport logging\nimport os\nimport re\n\nfrom io import BytesIO\nfrom tempfile import TemporaryFile\n\nimport boto3\nimport botocore\nimport time\nfrom warcio.archiveiterator import ArchiveIterator\nfrom warcio.recordloader import ArchiveLoadFailed\n\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SQLContext, SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType\nfrom multiprocessing.pool import ThreadPool\nimport re\n\nfrom collections import Counter\n\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, ArrayType\n\nfrom bs4 import BeautifulSoup\n\n\n\nLOGGING_FORMAT = '%(asctime)s %(levelname)s %(name)s: %(message)s'\n\nclass JupyterCCSparkJob(object):\n    \"\"\"\n    A simple Spark job definition to process Common Crawl data\n    \"\"\"\n\n    name = 'CCSparkJob'\n\n    output_schema = StructType([\n        StructField(\"key\", StringType(), True),\n        StructField(\"val\", LongType(), True)\n    ])\n\n    # description of input and output shown in --help\n    input_descr = \"Path to file listing input paths\"\n    output_descr = \"Name of output table (saved in spark.sql.warehouse.dir)\"\n\n    warc_parse_http_header = True\n\n    args = None\n    records_processed = None\n    warc_input_processed = None\n    warc_input_failed = None\n    log_level = 'INFO'\n    logging.basicConfig(level=log_level, format=LOGGING_FORMAT)\n\n\n    num_input_partitions = 400\n    num_output_partitions = 80\n    input_path = \"dbfs:/FileStore/tables/hackathon_warc_feed.txt\"\n    output_path = \"hackathon_data_year\"\n\n\n    def parse_arguments(self):\n        \"\"\" Returns the parsed arguments from the command line \"\"\"\n\n        description = self.name\n        if self.__doc__ is not None:\n            description += \" - \"\n            description += self.__doc__\n        arg_parser = argparse.ArgumentParser(prog=self.name, description=description,\n                                             conflict_handler='resolve')\n\n        arg_parser.add_argument(\"--input\", default=self.input_path, help=self.input_descr)\n        arg_parser.add_argument(\"--output\", default=self.output_path , help=self.output_descr)\n\n        arg_parser.add_argument(\"--num_input_partitions\", type=int,\n                                default=self.num_input_partitions,\n                                help=\"Number of input splits/partitions\")\n        arg_parser.add_argument(\"--num_output_partitions\", type=int,\n                                default=self.num_output_partitions,\n                                help=\"Number of output partitions\")\n        arg_parser.add_argument(\"--output_format\", default=\"parquet\",\n                                help=\"Output format: parquet (default),\"\n                                \" orc, json, csv\")\n        arg_parser.add_argument(\"--output_compression\", default=\"gzip\",\n                                help=\"Output compression codec: None,\"\n                                \" gzip/zlib (default), snappy, lzo, etc.\")\n        arg_parser.add_argument(\"--output_option\", action='append', default=[],\n                                help=\"Additional output option pair\"\n                                \" to set (format-specific) output options, e.g.,\"\n                                \" `header=true` to add a header line to CSV files.\"\n                                \" Option name and value are split at `=` and\"\n                                \" multiple options can be set by passing\"\n                                \" `--output_option <name>=<value>` multiple times\")\n\n        arg_parser.add_argument(\"--local_temp_dir\", default=None,\n                                help=\"Local temporary directory, used to\"\n                                \" buffer content from S3\")\n\n        arg_parser.add_argument(\"--log_level\", default=self.log_level,\n                                help=\"Logging level\")\n        arg_parser.add_argument(\"--spark-profiler\", action='store_true',\n                                help=\"Enable PySpark profiler and log\"\n                                \" profiling metrics if job has finished,\"\n                                \" cf. spark.python.profile\")\n\n        self.add_arguments(arg_parser)\n        args = arg_parser.parse_args(args=[])\n        if not self.validate_arguments(args):\n            raise Exception(\"Arguments not valid\")\n        self.init_logging(args.log_level)\n        return args\n\n    def add_arguments(self, parser):\n        pass\n\n    def validate_arguments(self, args):\n        if \"orc\" == args.output_format and \"gzip\" == args.output_compression:\n            # gzip for Parquet, zlib for ORC\n            args.output_compression = \"zlib\"\n        return True\n\n    def get_output_options(self):\n        return {x[0]: x[1] for x in map(lambda x: x.split('=', 1),\n                                        self.args.output_option)}\n\n    def init_logging(self, level=None):\n        if level is None:\n            level = self.log_level\n        else:\n            self.log_level = level\n        logging.basicConfig(level=level, format=LOGGING_FORMAT)\n\n    def init_accumulators(self, sc):\n        self.records_processed = sc.accumulator(0)\n        self.warc_input_processed = sc.accumulator(0)\n        self.warc_input_failed = sc.accumulator(0)\n\n    def get_logger(self, spark_context=None):\n        \"\"\"Get logger from SparkContext or (if None) from logging module\"\"\"\n        if spark_context is None:\n            return logging.getLogger(self.name)\n        return spark_context._jvm.org.apache.log4j.LogManager \\\n            .getLogger(self.name)\n\n    def run(self):\n        self.args = self.parse_arguments()\n        \n        conf = SparkConf()\n\n        if self.args.spark_profiler:\n            conf = conf.set(\"spark.python.profile\", \"true\")\n\n        sc = SparkContext.getOrCreate(#appName=self.name,\n          \n            conf=conf)\n        \n        sqlc = SQLContext(sparkContext=sc)\n\n        self.init_accumulators(sc)\n\n        self.run_job(sc, sqlc)\n\n        if self.args.spark_profiler:\n            sc.show_profiles()\n\n        sc.stop()\n\n    def log_aggregator(self, sc, agg, descr):\n        self.get_logger(sc).info(descr.format(agg.value))\n\n    def log_aggregators(self, sc):\n        self.log_aggregator(sc, self.warc_input_processed,\n                            'WARC/WAT/WET input files processed = {}')\n        self.log_aggregator(sc, self.warc_input_failed,\n                            'WARC/WAT/WET input files failed = {}')\n        self.log_aggregator(sc, self.records_processed,\n                            'WARC/WAT/WET records processed = {}')\n\n    @staticmethod\n    def reduce_by_key_func(a, b):\n        return a + b\n\n    def run_job(self, sc, sqlc):\n       \n        input_data = sc.textFile(self.args.input,\n                                 minPartitions=self.args.num_input_partitions)\n        print(\"going to output process warcs\")\n        output = input_data.mapPartitionsWithIndex(self.process_warcs)\n        sqlc.createDataFrame(output, schema=self.output_schema) \\\n            .coalesce(self.args.num_output_partitions) \\\n            .write \\\n            .format(self.args.output_format) \\\n            .option(\"compression\", self.args.output_compression) \\\n            .options(**self.get_output_options()) \\\n            .saveAsTable(self.args.output)\n\n        self.log_aggregators(sc)\n    \n    def process_warcs(self, id_, iterator):\n        s3pattern = re.compile('^s3://([^/]+)/(.+)')\n        base_dir = \"/user/\"\n\n        # S3 client (not thread-safe, initialize outside parallelized loop)\n        no_sign_request = botocore.client.Config(\n            signature_version=botocore.UNSIGNED)\n        s3client = boto3.client('s3', config=no_sign_request)\n       \n        for uri in iterator:\n            self.warc_input_processed.add(1)\n            if uri.startswith('s3://'):\n                self.get_logger().info('Reading from S3 {}'.format(uri))\n                s3match = s3pattern.match(uri)\n                if s3match is None:\n                    self.get_logger().error(\"Invalid S3 URI: \" + uri)\n                    continue\n                bucketname = s3match.group(1)\n                path = s3match.group(2)\n                warctemp = TemporaryFile(mode='w+b',\n                                         dir=self.args.local_temp_dir)\n                try:\n                    s3client.download_fileobj(bucketname, path, warctemp)\n                except botocore.client.ClientError as exception:\n                    self.get_logger().error(\n                        'Failed to download {}: {}'.format(uri, exception))\n                    self.warc_input_failed.add(1)\n                    warctemp.close()\n                    continue\n                warctemp.seek(0)\n                stream = warctemp\n            elif uri.startswith('hdfs://'):\n                self.get_logger().error(\"HDFS input not implemented: \" + uri)\n                continue\n            else:\n                self.get_logger().info('Reading local stream {}'.format(uri))\n                if uri.startswith('file:'):\n                    uri = uri[5:]\n                uri = os.path.join(base_dir, uri)\n                try:\n                    stream = open(uri, 'rb')\n                except IOError as exception:\n                    self.get_logger().error(\n                        'Failed to open {}: {}'.format(uri, exception))\n                    self.warc_input_failed.add(1)\n                    continue\n\n            no_parse = (not self.warc_parse_http_header)\n           \n            try:\n                archive_iterator = ArchiveIterator(stream,\n                                                   no_record_parse=no_parse)\n                \n                for res in self.iterate_records(uri, archive_iterator):\n\n                    yield res\n            except ArchiveLoadFailed as exception:\n                self.warc_input_failed.add(1)\n                self.get_logger().error(\n                    'Invalid WARC: {} - {}'.format(uri, exception))\n            finally:\n                stream.close()\n\n    def process_record(self, record):\n        raise NotImplementedError('Processing record needs to be customized')\n\n    def iterate_records(self, _warc_uri, archive_iterator):\n        \"\"\"Iterate over all WARC records. This method can be customized\n           and allows to access also values from ArchiveIterator, namely\n           WARC record offset and length.\"\"\"\n    \n        for record in archive_iterator:\n            for res in self.process_record(record):\n              \n            \n                yield res\n         \n\n            self.records_processed.add(1)\n            # WARC record offset and length should be read after the record\n            # has been processed, otherwise the record content is consumed\n            # while offset and length are determined:\n            #  warc_record_offset = archive_iterator.get_record_offset()\n            #  warc_record_length = archive_iterator.get_record_length()\n\n    @staticmethod\n    def is_wet_text_record(record):\n        \"\"\"Return true if WARC record is a WET text/plain record\"\"\"\n        return (record.rec_type == 'conversion' and\n                record.content_type == 'text/plain')\n\n    @staticmethod\n    def is_wat_json_record(record):\n        \"\"\"Return true if WARC record is a WAT record\"\"\"\n        return (record.rec_type == 'metadata' and\n                record.content_type == 'application/json')\n\n    @staticmethod\n    def is_html(record):\n        \"\"\"Return true if (detected) MIME type of a record is HTML\"\"\"\n        html_types = ['text/html', 'application/xhtml+xml']\n        if (('WARC-Identified-Payload-Type' in record.rec_headers) and\n            (record.rec_headers['WARC-Identified-Payload-Type'] in\n             html_types)):\n            return True\n        for html_type in html_types:\n            if html_type in record.content_type:\n                return True\n        return False\n\n  \nclass StringMatchCountJob(JupyterCCSparkJob):\n    \"\"\" Word count (frequency list) from texts in Common Crawl WET files\"\"\"\n\n    name = \"StringMatchCount\"\n\n    output_schema = StructType([\n        StructField(\"title\", StringType(), True),\n        StructField(\"body\", StringType(), True),\n        StructField(\"url\", StringType(), True),\n        StructField(\"record_date\", StringType(), True),\n        StructField(\"content_length\", StringType(), True),\n        StructField(\"warc_ip\", StringType(), True),\n        StructField(\"warc_truncated\", StringType(), True),\n\n        StructField(\"server_name\", StringType(), True),\n        StructField(\"vary\", StringType(), True),\n        StructField(\"cache_control\", StringType(), True),\n        StructField(\"content_type\", StringType(), True),\n        StructField(\"http_date\", StringType(), True),\n        StructField(\"http_date_expires\", StringType(), True),\n        StructField(\"html_meta\", ArrayType(StringType()), True),\n        StructField(\"html_hyperlink\", ArrayType(StringType()), True),\n        StructField(\"html_image\", ArrayType(StringType()), True)\n    ]\n\n    )\n\n    # simple Unicode-aware tokenization\n    # (not suitable for CJK languages)\n\n    word_pattern = re.compile('\\w+', re.UNICODE)\n    \n    def process_record(self, record):\n        #check domain\n        proceed = False\n        temp_url = record.rec_headers.get_header('WARC-Target-URI',None)\n        if not temp_url:\n          return\n        for n in list_of_domains:\n          if n in temp_url:\n            proceed = True\n            break\n        proceed = True\n        if not proceed:\n          return\n        \n        if record.rec_type == 'response':\n            # record headers\n\n            record_url = record.rec_headers.get_header('WARC-Target-URI', None)\n            record_date = record.rec_headers.get_header('WARC-Date', None)\n            record_content_length = record.rec_headers.get_header('Content-Length', None)\n            record_ip = record.rec_headers.get_header('WARC-IP-Address', None)\n            record_truncated = record.rec_headers.get_header('WARC-Truncated', None)\n\n            # http headers\n\n            http_server_name = record.http_headers.get_header('Server', None)\n            http_vary = record.http_headers.get_header('Vary', None)\n            http_cache_control = record.http_headers.get_header('Cache-Control', None)\n            http_content_type = record.http_headers.get_header('Content-Type', None)\n            http_date = record.http_headers.get_header('Date', None)\n            http_expires = record.http_headers.get_header('Expires', None)\n\n            data = record.content_stream().read()\n\n            soup = BeautifulSoup(data, 'lxml')\n\n\n            title = soup.find('title').string if soup.find('title') else None\n            if not title:\n                return\n            paragraphs = soup.find_all(['p', 'b'])\n\n            whitelist = ['meta', 'a', 'img']\n            html_meta = []\n            html_hyperlink = []\n            html_image = []\n            for t in soup.find_all(whitelist):\n                if t.name == 'meta':\n                    html_meta.append(str(t))\n                elif t.name == 'a':\n                    html_hyperlink.append(str(t))\n                elif t.name == 'img':\n                    html_image.append(str(t))\n\n            body = \"\"\n            for paragraph in paragraphs:\n                if not paragraph.has_attr('class'):\n                    body += paragraph.getText() + \" \"\n\n            string_title = str(title)\n\n            yield string_title, body, record_url, record_date, record_content_length, record_ip,\\\n                  record_truncated, http_server_name, http_vary, http_cache_control, http_content_type, \\\n                  http_date, http_expires, html_meta, html_hyperlink, html_image\n        else:\n            return\n#dbutils.fs.rm('/user/hive/warehouse/hackathon_data_year', True)\njob = StringMatchCountJob()\n\njob.run()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["#Populate list_of_companies with a list of the company names\n\nlist_of_companies = []\ncompany_feed = \"/FileStore/tables/all_labeled_data.csv\"\nsubmarkets = spark.read.format('csv').options(header='true', inferSchema='true').load(company_feed)\nfor row in submarkets.collect():\n  list_of_companies.append(row['company_name'])"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["#The dataframe that is directly created from the WARC file scanner\nsource_dataframe_path = \"FileStore/tables/filtered_hackathon_may_2020\"\noutput_path = \"s3://gp-databricks-files/news_data/may_data\""],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["import os\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SQLContext, SparkSession\nfrom pyspark.sql.functions import udf, col, lit\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, BooleanType\nimport argparse\nimport spacy\nimport pandas as pd\nfrom tqdm import tqdm\nimport time\nfrom langdetect import detect\n\nconf = SparkConf()\nsc = SparkContext.getOrCreate(conf=conf)\nsqlc = SQLContext(sparkContext=sc)\ndf = sqlc.read.parquet(source_dataframe_path)\nnlp = spacy.load('en_core_web_sm')\nlogging.getLogger(\"py4j\").setLevel(logging.ERROR)\n\n  \n\n#Name entity recognition with spacy, on the title\ndef perform_NER_title(title, query):\n    doc = nlp(title)\n    for ent in doc.ents:\n        if ent.label_ == \"ORG\" and ent.text.lower() in query.lower():\n            return True\n    return False\n\n#Name entity recognition with spacy, on the body\ndef perform_NER_body(body, query):\n    doc = nlp(body)\n    for ent in doc.ents:\n        if ent.label_ == \"ORG\" and ent.text.lower() in query.lower():\n            return True\n    return False\n\n#We check if any of the seed domains match with the current record, if it matches that means it's likely that it is important\ndef domain_match(url):\n    for n in list_of_domains:\n        if n in url:\n            return True\n    return False\n  \n#Tokenize the title string and check if any of the tokens is equal to our company of interest\ndef token_match_title(title, query):\n    title_tokens = title.split()\n    for n in title_tokens:\n        if n == query:\n            return True\n    return False\n\n#Tokenize the body string and check if any of the tokens is equal to our company of interest\ndef token_match_body(body, query):\n    body_tokens = body.split()\n    for n in body_tokens:\n        if n == query:\n            return True\n    return False\n\n#Substring match on the title string\ndef string_match_title(title, query):\n    if query in title:\n        return True\n    return False\n\n#Substring match on the body string\ndef string_match_body(body, query):\n    if query in body:\n        return True\n    return False\n\n#We know that substring match on body / title supersets everything else, so we perform this initial filter to reduce search space of other filters\ndef performFilter(title, body, query):\n    if string_match_title(title,query):\n        return True\n    return False\n  \n  \n    if string_match_body(body, query) or string_match_title(title, query): #body and title string match supercedes all\n        return True\n    return False\n\n#Returning which company that we are matching on\ndef company_match(query):\n    return query\n  \ndef detect_language(title):\n  try:\n    lang = detect(title)\n  except Exception as e:\n    return \"No Lang\"\n \n  return lang\n\n#UDF definitions\nfilter_udf = udf(performFilter, BooleanType())\ndomain_match_udf = udf(domain_match, BooleanType())\ntitle_match_token_udf = udf(token_match_title, BooleanType())\nbody_match_token_udf = udf(token_match_body, BooleanType())\ntitle_match_string_udf = udf(string_match_title, BooleanType())\nbody_match_string_udf = udf(string_match_body, BooleanType())\nner_title_udf = udf(perform_NER_title, BooleanType())\nner_body_udf = udf(perform_NER_body, BooleanType())\ncompany_match_udf = udf(company_match, StringType())\ndetect_language_udf = udf(detect_language, StringType())\n\nprocessed_companies = set()\n\ndef initial_filter(title, body):\n  for company_name in list_of_companies:\n    if company_name in title or company_name in body:\n      return True\n  return False\n  \ninitial_filter_udf = udf(initial_filter, BooleanType())\ndf = df.filter(initial_filter_udf(\"title\",\"body\"))\n\n\ndef performQuery(n, df):\n    #saves a parquet for a single company with name n\n    path_n = n.replace(\" \",\"_\")\n    tmp_path = f\"{output_path}/{path_n}\"\n    \n    if path_n in processed_companies:\n      print(f\"{n} already saved!\")\n      return True\n    print(path_n)\n    \n    master_df = None\n    start = time.time()\n    #Add in bool columns for each type of filter\n     \n    filtered_df = df.filter(df.body.contains(n) | df.title.contains(n) )\n    \n    filtered_df = filtered_df.withColumn(\"domain_match\", domain_match_udf(\"url\"))\n    filtered_df = filtered_df.withColumn(\"title_match_string\", title_match_string_udf(\"title\", lit(n)))\n    filtered_df = filtered_df.withColumn(\"body_match_string\", body_match_string_udf(\"body\", lit(n)))\n    filtered_df = filtered_df.withColumn(\"title_match_token\", title_match_token_udf(\"title\", lit(n)))\n    filtered_df = filtered_df.withColumn(\"body_match_token\", body_match_token_udf(\"body\", lit(n)))\n    filtered_df = filtered_df.withColumn(\"title_ner\", ner_title_udf(\"title\", lit(n)))\n    filtered_df = filtered_df.withColumn(\"language\", detect_language_udf(\"title\"))\n    \n    #filtered = filtered_df.withColumn(\"body_ner\", ner_body_udf(\"body\", lit(n)))\n    #print(\"Done body_ner: \", time.time() - start)\n    \n    filtered_df = filtered_df.withColumn(\"company_match\", company_match_udf(lit(n)))\n    master_df = filtered_df\n    if master_df.count() > 0:\n      master_df.write.parquet(tmp_path)\n    processed_companies.add(path_n)\n    print(f\"Done saving to csv for {n}\", time.time() - start)\n    return True\n\nfor n in list_of_companies:\n  performQuery(n, df)\n\n\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o43866.parquet.\n: org.apache.spark.sql.AnalysisException: path s3://gp-databricks-files/news_data/may_data/Alt/S already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:148)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:126)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:150)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:191)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:187)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:117)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:115)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:712)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:712)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1$$anonfun$apply$1.apply(SQLExecution.scala:112)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:74)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:184)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:712)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:308)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:294)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:236)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:602)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-760623353955491&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    218</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    219</span> <span class=\"ansi-green-fg\">for</span> n <span class=\"ansi-green-fg\">in</span> list_of_companies<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 220</span><span class=\"ansi-red-fg\">   </span>performQuery<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> df<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    221</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    222</span> print<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;full time:&#34;</span> <span class=\"ansi-blue-fg\">,</span>time<span class=\"ansi-blue-fg\">.</span>time<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">-</span> realstart<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">&lt;command-760623353955491&gt;</span> in <span class=\"ansi-cyan-fg\">performQuery</span><span class=\"ansi-blue-fg\">(n, df)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    204</span>     master_df <span class=\"ansi-blue-fg\">=</span> filtered_df\n<span class=\"ansi-green-intense-fg ansi-bold\">    205</span>     <span class=\"ansi-green-fg\">if</span> master_df<span class=\"ansi-blue-fg\">.</span>count<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">&gt;</span> <span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 206</span><span class=\"ansi-red-fg\">       </span>master_df<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span>tmp_path<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    207</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    208</span>     <span class=\"ansi-red-fg\">#master_df.repartition(32).write.parquet(tmp_path)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">parquet</span><span class=\"ansi-blue-fg\">(self, path, mode, partitionBy, compression)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    842</span>             self<span class=\"ansi-blue-fg\">.</span>partitionBy<span class=\"ansi-blue-fg\">(</span>partitionBy<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    843</span>         self<span class=\"ansi-blue-fg\">.</span>_set_opts<span class=\"ansi-blue-fg\">(</span>compression<span class=\"ansi-blue-fg\">=</span>compression<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 844</span><span class=\"ansi-red-fg\">         </span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    845</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    846</span>     <span class=\"ansi-blue-fg\">@</span>since<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1.6</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#39;path s3://gp-databricks-files/news_data/may_data/Alt/S already exists.;&#39;</div>"]}}],"execution_count":7}],"metadata":{"name":"sparkcc","notebookId":1396796701613468},"nbformat":4,"nbformat_minor":0}
